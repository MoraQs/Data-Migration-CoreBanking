{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define PostgreSQL Connection String (Source and Destination)\n",
    "POSTGRES_CONN_STR = (\n",
    "    f\"postgresql+psycopg://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}\"\n",
    "    f\"@{os.getenv('POSTGRES_HOST', 'localhost')}:{os.getenv('POSTGRES_PORT', '5432')}/{os.getenv('POSTGRES_DB')}\"\n",
    ")\n",
    "\n",
    "\n",
    "POSTGRES_DEST_CONN_STR = (\n",
    "    f\"postgresql+psycopg://{os.getenv('POSTGRES_USER_DEST')}:{os.getenv('POSTGRES_PASSWORD_DEST')}\"\n",
    "    f\"@{os.getenv('POSTGRES_HOST_DEST', 'localhost')}:{os.getenv('POSTGRES_PORT_DEST', '5432')}/{os.getenv('POSTGRES_DB_DEST')}\"\n",
    ")\n",
    "\n",
    "# Create SQLAlchemy Engine\n",
    "source_engine = create_engine(POSTGRES_CONN_STR)\n",
    "destination_engine = create_engine(POSTGRES_DEST_CONN_STR)\n",
    "\n",
    "# Configure logging\n",
    "log_dir = os.path.join(os.getcwd(), \"log\")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "log_file = os.path.join(log_dir, \"data_migration.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, mode=\"a\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a session for transactions\n",
    "Session = sessionmaker(bind=source_engine)\n",
    "session = Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract data from staging DB\n",
    "def extract_staging_data():\n",
    "    \"\"\"Fetch data from staging DB.\"\"\"\n",
    "    tables = [\"stg_customers\", \"customer_uuids\"]\n",
    "    dataframes = {}\n",
    "\n",
    "    with source_engine.connect() as conn:\n",
    "        for table in tables:\n",
    "            query = f\"SELECT * FROM {table};\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "            dataframes[table] = df\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "df_extracted = extract_staging_data()\n",
    "stg_customers = df_extracted[\"stg_customers\"]\n",
    "customer_uuids = df_extracted[\"customer_uuids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mapping document\n",
    "def load_mapping():\n",
    "    \"\"\"Load mapping document and return relevant dictionaries.\"\"\"\n",
    "\n",
    "    # Mapping Document Path\n",
    "    mapping_file = \"mapping_doc/migration_mapping_doc.xlsx\"\n",
    "\n",
    "    # Load Individual and Corporate mappings\n",
    "    df_ind_mapping = pd.read_excel(mapping_file, sheet_name=\"Customer Profile Individual\")\n",
    "    df_corp_mapping = pd.read_excel(mapping_file, sheet_name=\"Customer Profile Corporate\")\n",
    "\n",
    "    # Load JSON Field sheets\n",
    "    df_json_ind = pd.read_excel(mapping_file, sheet_name=\"JSON Field Individual\")\n",
    "    df_json_corp = pd.read_excel(mapping_file, sheet_name=\"JSON Field Corporate\")\n",
    "\n",
    "    # Convert mappings to dictionaries\n",
    "    ind_map = {k:v for k, v in zip(df_ind_mapping[\"Source Field\"], df_ind_mapping[\"Destination Field\"]) if pd.notna(k)}\n",
    "    corp_map = {k:v for k, v in zip(df_corp_mapping[\"Source Field\"], df_corp_mapping[\"Destination Field\"]) if pd.notna(k)}\n",
    "\n",
    "    # Default Values\n",
    "    ind_defaults = {k: v for k, v in zip(df_ind_mapping[\"Destination Field\"], df_ind_mapping[\"Default Value\"]) if pd.notna(k)}\n",
    "    corp_defaults = {k: v for k, v in zip(df_corp_mapping[\"Destination Field\"], df_corp_mapping[\"Default Value\"]) if pd.notna(k)}\n",
    "\n",
    "    # Extract JSON Fields in the **exact order** from the document\n",
    "    json_ind_fields = df_json_ind[\"Destination Field\"].dropna().tolist()  #  Preserves ordinal order in Excel Mapping Document\n",
    "    json_corp_fields = df_json_corp[\"Destination Field\"].dropna().tolist()  #  Preserves ordinal order in Excel Mapping Document\n",
    "\n",
    "    return ind_map, corp_map, ind_defaults, corp_defaults, json_ind_fields, json_corp_fields\n",
    "\n",
    "# Load mapping document\n",
    "ind_map, corp_map, ind_defaults, corp_defaults, json_ind_fields, json_corp_fields = load_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_json(row, json_fields, defaults):\n",
    "    \"\"\"Constructs the JSON structure for `customerProfileData`, ensuring:\n",
    "\n",
    "    - Date fields are converted to string format (YYYY-MM-DD).\n",
    "    - Missing values (NaT, None) are replaced with an empty string \"\".\n",
    "    \"\"\"\n",
    "    structured_json = {} # Use a dictionary to maintain order\n",
    "\n",
    "    for field in json_fields:  # Maintain order from Excel\n",
    "        value = row.get(field, defaults.get(field, \"\"))\n",
    "\n",
    "        # Convert date, and Timestamp to string format\n",
    "        if isinstance(value, (pd.Timestamp)):\n",
    "            value = value.strftime('%Y-%m-%d')  # Correct datetime handling\n",
    "\n",
    "        # Handle missing values properly\n",
    "        if pd.isna(value):\n",
    "            value = \"\"\n",
    "\n",
    "        # Ensure JSON contains only serializable data\n",
    "        if not isinstance(value, (str, int, float, bool, list, dict, type(None))):\n",
    "            value = str(value)\n",
    "\n",
    "        structured_json[field] = value  # Assign value to JSON field\n",
    "\n",
    "    return structured_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Transform Data\n",
    "def transform_data(stg_customers_df, customer_uuids_df):\n",
    "    \"\"\"Join extracted data with pre-generated UUID mapping and apply transformations.\"\"\"\n",
    "\n",
    "    df = pd.merge(stg_customers_df, customer_uuids_df, on='customer_code', how='left')\n",
    "\n",
    "    # Split DataFrame into Individual & Corporate\n",
    "    df_ind = df[df[\"customer_type\"] == \"Individual\"].copy()\n",
    "    df_corp = df[df[\"customer_type\"] == \"SME\"].copy()\n",
    "\n",
    "    # Apply Field Mappings\n",
    "    df_ind.rename(columns=ind_map, inplace=True)\n",
    "    df_corp.rename(columns=corp_map, inplace=True)\n",
    "\n",
    "    # Define required columns from mapping\n",
    "    all_ind_columns = set(ind_map.values()).union(set(ind_defaults.keys()))\n",
    "    all_corp_columns = set(corp_map.values()).union(set(corp_defaults.keys()))\n",
    "\n",
    "\n",
    "    # Ensure all required columns exist in the DataFrame for individual and corporate\n",
    "    for column in all_ind_columns:\n",
    "        if column not in df_ind.columns:\n",
    "            df_ind[column] = ind_defaults.get(column, \"\")\n",
    "\n",
    "    for column in all_corp_columns:\n",
    "        if column not in df_corp.columns:\n",
    "            df_corp[column] = corp_defaults.get(column, \"\")\n",
    "\n",
    "    # Filter columns based on \"Destination Field\" in the mapping document\n",
    "    df_ind = df_ind[list(all_ind_columns)]\n",
    "    df_corp = df_corp[list(all_corp_columns)]\n",
    "  \n",
    "\n",
    "    # Apply JSON transformation to both dataframes (Individual & Corporate)\n",
    "    df_ind[\"customerProfileData\"] = df_ind.apply(lambda x: build_json(x, json_ind_fields, ind_defaults), axis=1)\n",
    "    df_corp[\"customerProfileData\"] = df_corp.apply(lambda x: build_json(x, json_corp_fields, corp_defaults), axis=1)\n",
    "\n",
    "    # Consolidate Individual & Corporate DataFrames into a single DataFrame\n",
    "    df_final = pd.concat([df_ind, df_corp], ignore_index=True)\n",
    "\n",
    "    # Convert datetime fields to pandas datetime\n",
    "    datetime_columns = ['createdAt', 'updatedAt']\n",
    "    for col in datetime_columns:\n",
    "        if col in df_final.columns:\n",
    "            df_final[col] = pd.to_datetime(df_final[col], errors='coerce')  # Convert to datetime\n",
    "\n",
    "\n",
    "    # Convert integer fields to string where required (PostgreSQL expects text for certain columns)\n",
    "    int_to_str_columns = [\"customerNumber\", \"bvn\"]\n",
    "    for col in int_to_str_columns:\n",
    "        if col in df_final.columns:\n",
    "            df_final[col] = df_final[col].astype(str)\n",
    "\n",
    "    # Drop index to prevent misalignment\n",
    "    df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "import logging\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "\n",
    "\n",
    "# Load data into PostgreSQL using SQLAlchemy connection\n",
    "def load_data(df_final):\n",
    "    \"\"\"Load transformed data into PostgreSQL using SQLAlchemy.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"📥 Loading data into Destination...\")\n",
    "        \n",
    "        # Fetch valid columns from the destination table\n",
    "        with destination_engine.connect() as conn:\n",
    "            query = text(\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = 'customer_profile'\n",
    "            \"\"\")\n",
    "            columns = conn.execute(query).fetchall()\n",
    "            valid_columns = [col[0] for col in columns]  # List of valid columns in destination table\n",
    "\n",
    "            # Ensure only valid columns are inserted\n",
    "            df_final = df_final[[col for col in df_final.columns if col in valid_columns]]\n",
    "\n",
    "            # Ensure missing columns in the DataFrame are filled with default values\n",
    "            for col in valid_columns:\n",
    "                if col not in df_final.columns:\n",
    "                    df_final[col] = None  # Default value (can be set to a specific value like \"Unknown\")\n",
    "\n",
    "            # Define PostgreSQL column type mappings (to handle UUID and JSONB)\n",
    "            dtype_map = {\n",
    "                \"customerId\": UUID,          # Cast customerId as UUID\n",
    "                \"customerProfileId\": UUID,   # Cast customerProfileId as UUID\n",
    "                \"customerProfileData\": JSONB # Cast JSONB correctly\n",
    "            }\n",
    "\n",
    "            # Delete Existing Records Before Insert\n",
    "            with destination_engine.begin() as conn:\n",
    "                conn.execute(\n",
    "                    text(\"\"\"TRUNCATE TABLE customer_profile;\"\"\")\n",
    "                )\n",
    "            \n",
    "            # Bulk Insert Data using pandas to_sql method\n",
    "            df_final.to_sql(\n",
    "                \"customer_profile\",\n",
    "                destination_engine,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                dtype=dtype_map,  # Apply proper type mapping\n",
    "                method=\"multi\",   # Faster bulk insert\n",
    "                chunksize=1000\n",
    "            )\n",
    "\n",
    "        logging.info(\"✅ Data successfully inserted into customer_profile.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ An error occurred: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 20:45:18,603 - INFO - Starting data transfer process...\n",
      "2025-03-13 20:45:18,637 - INFO - 📌 Extracted 1000 records from stg_customers_df\n",
      "2025-03-13 20:45:18,641 - INFO - 📌 Extracted 1000 records from customer_uuids_df\n",
      "2025-03-13 20:45:19,079 - INFO - 📥 Loading data into Destination...\n",
      "2025-03-13 20:45:20,379 - INFO - ✅ Data successfully inserted into customer_profile.\n",
      "2025-03-13 20:45:20,379 - INFO - Start Datetime: 2025-03-13 20:45:18.603772\n",
      "2025-03-13 20:45:20,379 - INFO - End Datetime: 2025-03-13 20:45:20.379499\n",
      "2025-03-13 20:45:20,379 - INFO - ✅ Data transfer completed successfully in 0:00:01.775727.\n",
      "2025-03-13 20:45:20,379 - INFO - Total records inserted: 1000\n"
     ]
    }
   ],
   "source": [
    "# Run the ETL Pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to transfer data to PostgreSQL.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting data transfer process...\")\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # Extract\n",
    "        df_extracted = extract_staging_data()\n",
    "        stg_customers_df = df_extracted[\"stg_customers\"]\n",
    "        customer_uuids_df = df_extracted[\"customer_uuids\"]\n",
    "\n",
    "        logging.info(f\"📌 Extracted {len(stg_customers_df)} records from stg_customers_df\")\n",
    "        logging.info(f\"📌 Extracted {len(customer_uuids_df)} records from customer_uuids_df\")\n",
    "\n",
    "        # Transform\n",
    "        # Pass both dataframes to transform_data function\n",
    "        transformed_df = transform_data(stg_customers_df, customer_uuids_df)\n",
    "\n",
    "        # Load\n",
    "        load_data(transformed_df)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        logging.info(f\"Start Datetime: {start_time}\")\n",
    "        logging.info(f\"End Datetime: {end_time}\")\n",
    "        logging.info(f\"✅ Data transfer completed successfully in {end_time - start_time}.\")\n",
    "        logging.info(f\"Total records inserted: {len(transformed_df)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Data transfer failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
