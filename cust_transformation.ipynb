{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define PostgreSQL Connection String (Source and Destination)\n",
    "POSTGRES_CONN_STR = (\n",
    "    f\"postgresql+psycopg://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}\"\n",
    "    f\"@{os.getenv('POSTGRES_HOST', 'localhost')}:{os.getenv('POSTGRES_PORT', '5432')}/{os.getenv('POSTGRES_DB')}\"\n",
    ")\n",
    "\n",
    "POSTGRES_DEST_CONN_STR = (\n",
    "    f\"postgresql+psycopg://{os.getenv('POSTGRES_USER_DEST')}:{os.getenv('POSTGRES_PASSWORD_DEST')}\"\n",
    "    f\"@{os.getenv('POSTGRES_HOST_DEST', 'localhost')}:{os.getenv('POSTGRES_PORT_DEST', '5432')}/{os.getenv('POSTGRES_DB_DEST')}\"\n",
    ")\n",
    "\n",
    "# Create SQLAlchemy Engine\n",
    "source_engine = create_engine(POSTGRES_CONN_STR)\n",
    "destination_engine = create_engine(POSTGRES_DEST_CONN_STR)\n",
    "\n",
    "# Configure logging\n",
    "log_dir = os.path.join(os.getcwd(), \"log\")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "log_file = os.path.join(log_dir, \"data_migration.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, mode=\"a\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ‚úÖ Create a session for transactions\n",
    "Session = sessionmaker(bind=source_engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Field Mapping\n",
    "def load_mapping():\n",
    "    \"\"\"Load field mappings from the migration_mapping_document.\"\"\"\n",
    "    mapping_file = \"mapping_doc/migration_mapping_doc.xlsx\"\n",
    "    df_mapping = pd.read_excel(mapping_file, sheet_name=\"Customer Ind-Corporate\")\n",
    "\n",
    "    # Extract mappings: Source ‚Üí Destination, Default Values\n",
    "    field_map = {k: v for k, v in zip(df_mapping[\"Source Field\"], df_mapping[\"Destination Field\"]) if pd.notna(k)}\n",
    "    default_values = {k: v for k, v in zip(df_mapping[\"Destination Field\"], df_mapping[\"Default Value\"]) if pd.notna(k)}\n",
    "\n",
    "    return field_map, default_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract data from staging DB\n",
    "def extract_staging_data():\n",
    "    \"\"\"Fetch data from staging DB.\"\"\"\n",
    "    tables = [\"stg_customers\", \"customer_uuids\"]\n",
    "    dataframes = {}\n",
    "\n",
    "    with source_engine.connect() as conn:\n",
    "        for table in tables:\n",
    "            query = f\"SELECT * FROM {table};\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "            dataframes[table] = df\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "df_extracted = extract_staging_data()\n",
    "stg_customers = df_extracted[\"stg_customers\"]\n",
    "customer_uuids = df_extracted[\"customer_uuids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['branchId', 'CategoryOfCustomer', 'requiresRegularization',\n",
      "       'approverId', 'customerType', 'approvalStatus', 'CategoryOfBusiness',\n",
      "       'status', 'createdAt', 'branch', 'initiatorId', 'updatedAt',\n",
      "       'isAutoReactivityOnly', 'monitoring', 'approver', 'initiator',\n",
      "       'customerId', 'isTerminated', 'tenantId'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Transform Data\n",
    "def transform_data(stg_customers_df, customer_uuids_df):\n",
    "    \"\"\"Join extracted data with pre-generated UUID mapping and apply transformations.\"\"\"\n",
    "\n",
    "    df_all = pd.merge(stg_customers_df, customer_uuids_df, on='customer_code', how='left')\n",
    "\n",
    "    # Load Field Mapping\n",
    "    field_map, default_values = load_mapping()\n",
    "\n",
    "    # Apply Field Mappings\n",
    "    df_all.rename(columns=field_map, inplace=True)\n",
    "\n",
    "    # Define required columns from mapping\n",
    "    all_ind_columns = set(field_map.values()).union(set(default_values.keys()))\n",
    "\n",
    "\n",
    "    # Ensure all required columns exist in the DataFrame for individual and corporate\n",
    "    for column in all_ind_columns:\n",
    "        if column not in df_all.columns:\n",
    "            df_all[column] = default_values.get(column, \"\")\n",
    "\n",
    "\n",
    "    # Filter columns based on \"Destination Field\" in the mapping document\n",
    "    df_all = df_all[list(all_ind_columns)]\n",
    "\n",
    "\n",
    "    # Convert datetime fields to pandas datetime\n",
    "    datetime_columns = ['createdAt', 'updatedAt']\n",
    "    for col in datetime_columns:\n",
    "        if col in df_all.columns:\n",
    "            df_all[col] = pd.to_datetime(df_all[col], errors='coerce')  # Convert to datetime\n",
    "\n",
    "\n",
    "    # Drop index to prevent misalignment\n",
    "    df_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(df_all.columns)\n",
    "\n",
    "    return df_all\n",
    "\n",
    "df = transform_data(stg_customers, customer_uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "\n",
    "\n",
    "# Load Data into PostgreSQL\n",
    "def load_data(df):\n",
    "    \"\"\"Load transformed data into `customer` in PostgreSQL.\"\"\"\n",
    "\n",
    "    try:\n",
    "        logging.info(\"üì• Loading data into Destination...\")\n",
    "        \n",
    "        # Fetch valid columns from the destination table\n",
    "        with destination_engine.connect() as conn:\n",
    "            query = text(\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = 'customer'\n",
    "            \"\"\")\n",
    "            columns = conn.execute(query).fetchall()\n",
    "            valid_columns = [col[0] for col in columns]  # List of valid columns in destination table\n",
    "\n",
    "            # Ensure only valid columns are inserted\n",
    "            df = df[[col for col in df.columns if col in valid_columns]]\n",
    "\n",
    "            # Ensure missing columns in the DataFrame are filled with default values (e.g. None or specific defaults)\n",
    "            for col in valid_columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None  # Default value, can be a specific value like \"Unknown\"\n",
    "\n",
    "\n",
    "            # Delete Existing Records Before Insert\n",
    "            with destination_engine.begin() as conn:\n",
    "                conn.execute(\n",
    "                    text(\"\"\"TRUNCATE TABLE customer;\"\"\")\n",
    "                )\n",
    "\n",
    "            # Define PostgreSQL column type mappings (to handle UUID)\n",
    "            dtype_map = {\n",
    "                \"customerId\": UUID,\n",
    "                \"tenantId\": UUID,\n",
    "                \"approverId\": UUID,\n",
    "                \"initiatorId\": UUID,\n",
    "                \"branchId\": UUID\n",
    "            }\n",
    "\n",
    "            # Bulk Insert Data using pandas to_sql method\n",
    "            df.to_sql(\n",
    "                \"customer\",\n",
    "                destination_engine,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                dtype=dtype_map,  # Apply proper type mapping\n",
    "                method=\"multi\",   # Faster bulk insert\n",
    "                chunksize=1000\n",
    "            )\n",
    "\n",
    "        logging.info(\"‚úÖ Data successfully inserted into customer.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå An error occurred: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 21:20:03,533 - INFO - Starting data transfer process...\n",
      "2025-03-13 21:20:03,571 - INFO - üìå Extracted 1000 records from stg_customers_df\n",
      "2025-03-13 21:20:03,572 - INFO - üìå Extracted 1000 records from customer_uuids_df\n",
      "2025-03-13 21:20:03,609 - INFO - üì• Loading data into Destination...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['branchId', 'CategoryOfCustomer', 'requiresRegularization',\n",
      "       'approverId', 'customerType', 'approvalStatus', 'CategoryOfBusiness',\n",
      "       'status', 'createdAt', 'branch', 'initiatorId', 'updatedAt',\n",
      "       'isAutoReactivityOnly', 'monitoring', 'approver', 'initiator',\n",
      "       'customerId', 'isTerminated', 'tenantId'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 21:20:04,581 - INFO - ‚úÖ Data successfully inserted into customer.\n",
      "2025-03-13 21:20:04,582 - INFO - Start Datetime: 2025-03-13 21:20:03.535638\n",
      "2025-03-13 21:20:04,584 - INFO - End Datetime: 2025-03-13 21:20:04.582912\n",
      "2025-03-13 21:20:04,584 - INFO - ‚úÖ Data transfer completed successfully in 0:00:01.047274.\n",
      "2025-03-13 21:20:04,585 - INFO - Total records inserted: 1000\n"
     ]
    }
   ],
   "source": [
    "# Run the ETL Pipeline\n",
    "def main():\n",
    "    \"\"\"Main function to transfer data from SQL Server to PostgreSQL.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting data transfer process...\")\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # Extract\n",
    "        df_extracted = extract_staging_data()\n",
    "        stg_customers_df = df_extracted[\"stg_customers\"]\n",
    "        customer_uuids_df = df_extracted[\"customer_uuids\"]\n",
    "\n",
    "        logging.info(f\"üìå Extracted {len(stg_customers_df)} records from stg_customers_df\")\n",
    "        logging.info(f\"üìå Extracted {len(customer_uuids_df)} records from customer_uuids_df\")\n",
    "\n",
    "        # Transform\n",
    "        transformed_df = transform_data(stg_customers_df, customer_uuids_df)\n",
    "\n",
    "        # Load\n",
    "        load_data(transformed_df)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        logging.info(f\"Start Datetime: {start_time}\")\n",
    "        logging.info(f\"End Datetime: {end_time}\")\n",
    "        logging.info(f\"‚úÖ Data transfer completed successfully in {end_time - start_time}.\")\n",
    "        logging.info(f\"Total records inserted: {len(transformed_df)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Data transfer failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
